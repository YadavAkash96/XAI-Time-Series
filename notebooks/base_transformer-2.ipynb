{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8968869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('./cleaned_data/electric_screwdriver_dataset_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9eafe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_acc</th>\n",
       "      <th>X_gyr</th>\n",
       "      <th>X_mag</th>\n",
       "      <th>X_mic</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-5.877378, 1.322051, 8.612491], [-5.81989653...</td>\n",
       "      <td>[[-0.01065264, 0.01171791, -0.02130529], [-0.0...</td>\n",
       "      <td>[[0.0002260082, 5.341905e-05, -0.000693571], [...</td>\n",
       "      <td>[[7.0], [-1.0], [-9.0], [-6.0], [-9.0], [-11.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.7765572746195495, -3.3777170393376266, 10....</td>\n",
       "      <td>[[0.2405253416105745, -0.1589432131596917, 0.5...</td>\n",
       "      <td>[[-0.00017548574, 0.0002208949812538, 0.000350...</td>\n",
       "      <td>[[-176.0], [-222.0], [-295.0], [-398.0], [-479...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-4.661880464586655, 1.9574543543611933, 8.63...</td>\n",
       "      <td>[[-0.4239807449647005, -0.1303672858574356, 0....</td>\n",
       "      <td>[[0.0002206308612263, 5.172351082169609e-05, -...</td>\n",
       "      <td>[[20.0], [19.0], [16.0], [9.0], [9.0], [12.0],...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.2243101964783376, 0.8909580531877421, 6.9...</td>\n",
       "      <td>[[0.7246870594223318, 0.0347562930971468, 0.17...</td>\n",
       "      <td>[[-0.0001836712355924, 0.000201727285763, 0.00...</td>\n",
       "      <td>[[264.0], [325.0], [218.0], [4.0], [-334.0], [...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-4.654718747731929, 1.6489709338113354, 8.58...</td>\n",
       "      <td>[[-0.7671679936250123, -0.3679601965661887, 0....</td>\n",
       "      <td>[[0.0002186657774452, 5.5124810577497975e-05, ...</td>\n",
       "      <td>[[15.0], [7.0], [-7.0], [-14.0], [-18.0], [-11...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>[[-0.4435225921488645, 1.446229573999129, 9.67...</td>\n",
       "      <td>[[1.5150272513234182, 0.2443794430437472, 0.08...</td>\n",
       "      <td>[[-0.0001829688242856, 0.0002077097255544, 0.0...</td>\n",
       "      <td>[[32.0], [33.0], [30.0], [30.0], [26.0], [17.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>[[-0.3560756462892959, -3.229175592151793, 9.6...</td>\n",
       "      <td>[[-1.704929743579738, -0.2662585630831891, 0.1...</td>\n",
       "      <td>[[-0.0001818928115718, 0.0002072522331499, 0.0...</td>\n",
       "      <td>[[-12.0], [-8.0], [-6.0], [-7.0], [-10.0], [-1...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>[[-0.5858299366003309, -5.146209871058921, 9.0...</td>\n",
       "      <td>[[-1.1219787501838403, 0.1069062964748598, 0.0...</td>\n",
       "      <td>[[-0.0001823644517704, 0.0002003066688551, 0.0...</td>\n",
       "      <td>[[8.0], [25.0], [31.0], [-1.0], [-72.0], [-103...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>[[-0.144674183359998, -2.698030852888813, 9.14...</td>\n",
       "      <td>[[3.119370343340732, 0.0056913177676755, 0.446...</td>\n",
       "      <td>[[-0.0001829336829628, 0.0002016430804244, 0.0...</td>\n",
       "      <td>[[-91.0], [-93.0], [-88.0], [-54.0], [0.0], [4...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7176</th>\n",
       "      <td>[[-0.6124043738554599, 4.364567202346398, 9.08...</td>\n",
       "      <td>[[-0.7115167660601478, -0.0042009596290397, -0...</td>\n",
       "      <td>[[-0.0001793801893456, 0.0002081338750783, 0.0...</td>\n",
       "      <td>[[-2.0], [-3.0], [-4.0], [-11.0], [-17.0], [-2...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  X_acc  \\\n",
       "0     [[-5.877378, 1.322051, 8.612491], [-5.81989653...   \n",
       "1     [[0.7765572746195495, -3.3777170393376266, 10....   \n",
       "2     [[-4.661880464586655, 1.9574543543611933, 8.63...   \n",
       "3     [[-0.2243101964783376, 0.8909580531877421, 6.9...   \n",
       "4     [[-4.654718747731929, 1.6489709338113354, 8.58...   \n",
       "...                                                 ...   \n",
       "7172  [[-0.4435225921488645, 1.446229573999129, 9.67...   \n",
       "7173  [[-0.3560756462892959, -3.229175592151793, 9.6...   \n",
       "7174  [[-0.5858299366003309, -5.146209871058921, 9.0...   \n",
       "7175  [[-0.144674183359998, -2.698030852888813, 9.14...   \n",
       "7176  [[-0.6124043738554599, 4.364567202346398, 9.08...   \n",
       "\n",
       "                                                  X_gyr  \\\n",
       "0     [[-0.01065264, 0.01171791, -0.02130529], [-0.0...   \n",
       "1     [[0.2405253416105745, -0.1589432131596917, 0.5...   \n",
       "2     [[-0.4239807449647005, -0.1303672858574356, 0....   \n",
       "3     [[0.7246870594223318, 0.0347562930971468, 0.17...   \n",
       "4     [[-0.7671679936250123, -0.3679601965661887, 0....   \n",
       "...                                                 ...   \n",
       "7172  [[1.5150272513234182, 0.2443794430437472, 0.08...   \n",
       "7173  [[-1.704929743579738, -0.2662585630831891, 0.1...   \n",
       "7174  [[-1.1219787501838403, 0.1069062964748598, 0.0...   \n",
       "7175  [[3.119370343340732, 0.0056913177676755, 0.446...   \n",
       "7176  [[-0.7115167660601478, -0.0042009596290397, -0...   \n",
       "\n",
       "                                                  X_mag  \\\n",
       "0     [[0.0002260082, 5.341905e-05, -0.000693571], [...   \n",
       "1     [[-0.00017548574, 0.0002208949812538, 0.000350...   \n",
       "2     [[0.0002206308612263, 5.172351082169609e-05, -...   \n",
       "3     [[-0.0001836712355924, 0.000201727285763, 0.00...   \n",
       "4     [[0.0002186657774452, 5.5124810577497975e-05, ...   \n",
       "...                                                 ...   \n",
       "7172  [[-0.0001829688242856, 0.0002077097255544, 0.0...   \n",
       "7173  [[-0.0001818928115718, 0.0002072522331499, 0.0...   \n",
       "7174  [[-0.0001823644517704, 0.0002003066688551, 0.0...   \n",
       "7175  [[-0.0001829336829628, 0.0002016430804244, 0.0...   \n",
       "7176  [[-0.0001793801893456, 0.0002081338750783, 0.0...   \n",
       "\n",
       "                                                  X_mic  \\\n",
       "0     [[7.0], [-1.0], [-9.0], [-6.0], [-9.0], [-11.0...   \n",
       "1     [[-176.0], [-222.0], [-295.0], [-398.0], [-479...   \n",
       "2     [[20.0], [19.0], [16.0], [9.0], [9.0], [12.0],...   \n",
       "3     [[264.0], [325.0], [218.0], [4.0], [-334.0], [...   \n",
       "4     [[15.0], [7.0], [-7.0], [-14.0], [-18.0], [-11...   \n",
       "...                                                 ...   \n",
       "7172  [[32.0], [33.0], [30.0], [30.0], [26.0], [17.0...   \n",
       "7173  [[-12.0], [-8.0], [-6.0], [-7.0], [-10.0], [-1...   \n",
       "7174  [[8.0], [25.0], [31.0], [-1.0], [-72.0], [-103...   \n",
       "7175  [[-91.0], [-93.0], [-88.0], [-54.0], [0.0], [4...   \n",
       "7176  [[-2.0], [-3.0], [-4.0], [-11.0], [-17.0], [-2...   \n",
       "\n",
       "                                             y  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "...                                        ...  \n",
       "7172  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "7173  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "7174  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "7175  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "7176  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "\n",
       "[7177 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed747f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_acc = np.array(df['X_acc'])\n",
    "X_acc = np.stack(X_acc)\n",
    "\n",
    "X_gyr = np.array(df['X_gyr'])\n",
    "X_gyr = np.stack(X_gyr)\n",
    "\n",
    "X_mag = np.array(df['X_mag'])\n",
    "X_mag = np.stack(X_mag)\n",
    "\n",
    "X_mic = np.array(df['X_mic'])\n",
    "X_mic = np.stack(X_mic)\n",
    "\n",
    "y = np.array(df['y'])\n",
    "y = np.stack(y)\n",
    "\n",
    "print(type(X_acc), type(X_gyr),type(X_mag),type(X_mic), type(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert one-hot vectors to class labels\n",
    "# Assumes only one '1' per vector\n",
    "y_class = np.array([np.argmax(label) for label in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537888de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample class numbers: [8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "# Get original class numbers from one-hot encoding\n",
    "class_names = [2,3,4,5,6,7,8,14]  # Your actual class order\n",
    "y_class_numbers = np.array([class_names[np.argmax(vec)] for vec in y])\n",
    "\n",
    "print(\"Sample class numbers:\", y_class_numbers[:5])\n",
    "# Should output: [2,3,14,6,...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d8fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split per sensor\n",
    "X_acc_train, X_acc_test, y_acc_train, y_acc_test = train_test_split(X_acc, y, test_size=0.2, stratify=y_class, random_state=42)\n",
    "X_gyr_train, X_gyr_test, y_gyr_train, y_gyr_test = train_test_split(X_gyr, y, test_size=0.2, stratify=y_class, random_state=42)\n",
    "X_mag_train, X_mag_test, y_mag_train, y_mag_test = train_test_split(X_mag, y, test_size=0.2, stratify=y_class, random_state=42)\n",
    "X_mic_train, X_mic_test, y_mic_train, y_mic_test = train_test_split(X_mic, y, test_size=0.2, stratify=y_class, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8bb2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerometer shape: (5741, 41, 3)\n",
      "Microphone shape: (5741, 3200, 1)\n",
      "Gyroscope shape: (5741, 41, 3)\n",
      "Magnetometer shape: (5741, 62, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accelerometer shape:\", X_acc_train.shape) \n",
    "print(\"Microphone shape:\", X_mic_train.shape) \n",
    "print(\"Gyroscope shape:\", X_gyr_train.shape)\n",
    "print(\"Magnetometer shape:\", X_mag_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6449669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([ 657,  268,  238,  106,   54,   70, 4335,   13]))\n",
      "Test classes: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([ 164,   67,   60,   26,   14,   17, 1085,    3]))\n"
     ]
    }
   ],
   "source": [
    "valid_classes = {2:0, 3:1, 4:2, 5:3, 6:4, 7:5, 8:6, 14:7}\n",
    "\n",
    "def clean_labels(y_split):\n",
    "    \"\"\"Converts labels to 0-7 within each split\"\"\"\n",
    "    y_clean = np.full_like(y_split, -1, dtype=int)\n",
    "    for orig, mapped in valid_classes.items():\n",
    "        y_clean[y_split == orig] = mapped\n",
    "    return y_clean\n",
    "\n",
    "# Clean labels\n",
    "y_clean = clean_labels(y_class_numbers)\n",
    "\n",
    "# Filter invalid samples\n",
    "valid_mask = y_clean != -1\n",
    "X_acc_valid = X_acc[valid_mask]\n",
    "X_gyr_valid = X_gyr[valid_mask]\n",
    "X_mag_valid = X_mag[valid_mask]\n",
    "X_mic_valid = X_mic[valid_mask]\n",
    "y_valid = y_clean[valid_mask]\n",
    "\n",
    "# Stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(len(y_valid))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    stratify=y_valid,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply to all modalities\n",
    "X_acc_train = X_acc_valid[train_idx]\n",
    "X_acc_test = X_acc_valid[test_idx]\n",
    "X_gyr_train = X_gyr_valid[train_idx]\n",
    "X_gyr_test = X_gyr_valid[test_idx]\n",
    "X_mag_train = X_mag_valid[train_idx]\n",
    "X_mag_test = X_mag_valid[test_idx]\n",
    "X_mic_train = X_mic_valid[train_idx]\n",
    "X_mic_test = X_mic_valid[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_valid[train_idx]\n",
    "y_test = y_valid[test_idx]\n",
    "\n",
    "# Verify\n",
    "print(\"Train classes:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Test classes:\", np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1002c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting ADASYN resampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahoo\\OneDrive\\Desktop\\Master_Seminar\\ADLTS\\tool-tracking\\.env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADASYN failed: No samples will be generated with the provided ratio settings.\n",
      "Falling back to SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahoo\\OneDrive\\Desktop\\Master_Seminar\\ADLTS\\tool-tracking\\.env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resampled Class Distribution:\n",
      "Class 0: 657 samples (Original: 657)\n",
      "Class 1: 350 samples (Original: 268)\n",
      "Class 2: 350 samples (Original: 238)\n",
      "Class 3: 250 samples (Original: 106)\n",
      "Class 4: 250 samples (Original: 54)\n",
      "Class 5: 250 samples (Original: 70)\n",
      "Class 6: 4335 samples (Original: 4335)\n",
      "Class 7: 350 samples (Original: 13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "# 1. Prepare multimodal data for resampling\n",
    "# ------------------------------------------\n",
    "# Flatten each sensor's timesteps into features\n",
    "X_combined = np.concatenate([\n",
    "    X_acc_train.reshape(len(X_acc_train), -1),\n",
    "    X_gyr_train.reshape(len(X_gyr_train), -1),\n",
    "    X_mag_train.reshape(len(X_mag_train), -1),\n",
    "    X_mic_train.reshape(len(X_mic_train), -1)\n",
    "], axis=1)\n",
    "\n",
    "# 2. Configure sampling strategy\n",
    "# -------------------------------\n",
    "# Original class counts from user's data\n",
    "original_counts = {0: 657, 1: 268, 2: 238, 3: 106, 4: 54, 5: 70, 6: 4335, 7: 13}\n",
    "\n",
    "# Target counts (only increase where needed)\n",
    "sampling_strategy = {\n",
    "    1: 350,  # Current 268 → Target 300 (+32)\n",
    "    2: 350,  # Current 238 → Target 300 (+62)\n",
    "    3: 250,  # Current 106 → Target 200 (+94)\n",
    "    4: 250,  # Current 54 → Target 200 (+146)\n",
    "    5: 250,  # Current 70 → Target 200 (+130)\n",
    "    7: 350   # Current 13 → Target 300 (+287)\n",
    "}\n",
    "\n",
    "# 3. Perform resampling with fallback\n",
    "# ------------------------------------\n",
    "try:\n",
    "    # Try ADASYN first with reduced neighbors\n",
    "    print(\"Attempting ADASYN resampling...\")\n",
    "    adasyn = ADASYN(\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        n_neighbors=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_combined, y_train)\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"ADASYN failed: {e}\\nFalling back to SMOTE...\")\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        k_neighbors=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_combined, y_train)\n",
    "\n",
    "# 4. Reconstruct sensor-specific arrays\n",
    "# --------------------------------------\n",
    "def reconstruct_sensor(X_flat, original_shape):\n",
    "    \"\"\"Reshapes flattened array back to sensor's original shape\"\"\"\n",
    "    return X_flat.reshape(-1, *original_shape[1:])\n",
    "\n",
    "# Get original samples indices\n",
    "_, original_idx = np.unique(X_resampled, axis=0, return_index=True)\n",
    "original_idx = original_idx[original_idx < len(X_acc_train)]  # Ensure valid indices\n",
    "\n",
    "# Split into original and synthetic\n",
    "synthetic_mask = np.ones(len(X_resampled), dtype=bool)\n",
    "synthetic_mask[original_idx] = False\n",
    "X_synthetic = X_resampled[synthetic_mask]\n",
    "\n",
    "# Initialize sensor data storage\n",
    "sensor_shapes = {\n",
    "    'acc': X_acc_train.shape,\n",
    "    'gyr': X_gyr_train.shape,\n",
    "    'mag': X_mag_train.shape,\n",
    "    'mic': X_mic_train.shape\n",
    "}\n",
    "\n",
    "sensor_data = {\n",
    "    'acc': {'original': X_acc_train[original_idx], 'synthetic': None},\n",
    "    'gyr': {'original': X_gyr_train[original_idx], 'synthetic': None},\n",
    "    'mag': {'original': X_mag_train[original_idx], 'synthetic': None},\n",
    "    'mic': {'original': X_mic_train[original_idx], 'synthetic': None}\n",
    "}\n",
    "\n",
    "sensor_sizes = [np.prod(shape[1:]) for shape in sensor_shapes.values()]\n",
    "sensor_offsets = np.cumsum([0] + sensor_sizes[:-1])\n",
    "\n",
    "for i, (sensor, _) in enumerate(sensor_shapes.items()):\n",
    "    start = sensor_offsets[i]\n",
    "    end = start + sensor_sizes[i]\n",
    "    \n",
    "    # Extract and reshape synthetic data\n",
    "    synth_flat = X_synthetic[:, start:end]\n",
    "    sensor_data[sensor]['synthetic'] = reconstruct_sensor(synth_flat, sensor_shapes[sensor])\n",
    "\n",
    "# Combine original and synthetic for each sensor\n",
    "for sensor in sensor_data:\n",
    "    original = sensor_data[sensor]['original']\n",
    "    synthetic = sensor_data[sensor]['synthetic']\n",
    "    sensor_data[sensor]['resampled'] = np.concatenate([original, synthetic], axis=0)\n",
    "\n",
    "# 5. Update training data\n",
    "# ------------------------\n",
    "X_acc_train = sensor_data['acc']['resampled']\n",
    "X_gyr_train = sensor_data['gyr']['resampled']\n",
    "X_mag_train = sensor_data['mag']['resampled']\n",
    "X_mic_train = sensor_data['mic']['resampled']\n",
    "y_train = y_resampled\n",
    "\n",
    "# 6. Verify new distribution\n",
    "# ---------------------------\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nResampled Class Distribution:\")\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Class {cls}: {count} samples (Original: {original_counts.get(cls, 0)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef681c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def build_enhanced_sensor_branch(input_shape, name):\n",
    "    \"\"\"Enhanced sensor processing with multi-scale features and attention\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name=f'{name}_input')\n",
    "    \n",
    "    # Multi-scale temporal feature extraction\n",
    "    x1 = layers.Conv1D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    \n",
    "    x2 = layers.Conv1D(32, 5, padding='same', activation='relu')(inputs)\n",
    "    x2 = layers.BatchNormalization()(x2)\n",
    "    \n",
    "    x3 = layers.Conv1D(32, 7, padding='same', activation='relu')(inputs)\n",
    "    x3 = layers.BatchNormalization()(x3)\n",
    "    \n",
    "    # Concatenate multi-scale features\n",
    "    x = layers.Concatenate()([x1, x2, x3])\n",
    "    \n",
    "    # Self-attention mechanism\n",
    "    x = layers.MultiHeadAttention(\n",
    "        num_heads=4, \n",
    "        key_dim=24,\n",
    "        dropout=0.1\n",
    "    )(x, x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # Residual connection\n",
    "    res = layers.Conv1D(96, 1, padding='same')(inputs)\n",
    "    x = layers.Add()([x, res])\n",
    "    \n",
    "    # Global feature extraction\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=x, name=f'{name}_branch')\n",
    "\n",
    "def build_enhanced_fusion_model(input_shapes):\n",
    "    \"\"\"Enhanced multimodal fusion with cross-modal attention\"\"\"\n",
    "    # Input layers\n",
    "    acc_in = layers.Input(input_shapes['acc'], name='acc')\n",
    "    gyr_in = layers.Input(input_shapes['gyr'], name='gyr')\n",
    "    mag_in = layers.Input(input_shapes['mag'], name='mag')\n",
    "    mic_in = layers.Input(input_shapes['mic'], name='mic')\n",
    "\n",
    "    # Enhanced sensor branches\n",
    "    acc_features = build_enhanced_sensor_branch(input_shapes['acc'], 'acc')(acc_in)\n",
    "    gyr_features = build_enhanced_sensor_branch(input_shapes['gyr'], 'gyr')(gyr_in)\n",
    "    mag_features = build_enhanced_sensor_branch(input_shapes['mag'], 'mag')(mag_in)\n",
    "    mic_features = build_enhanced_sensor_branch(input_shapes['mic'], 'mic')(mic_in)\n",
    "    \n",
    "    # Cross-modal attention fusion\n",
    "    all_features = layers.Lambda(lambda x: tf.stack(x, axis=1))([\n",
    "        acc_features, gyr_features, mag_features, mic_features\n",
    "    ])\n",
    "    \n",
    "    # Cross-attention between modalities\n",
    "    attended = layers.MultiHeadAttention(\n",
    "        num_heads=4,\n",
    "        key_dim=24\n",
    "    )(all_features, all_features)\n",
    "    \n",
    "    # Flatten for classification\n",
    "    merged = layers.Flatten()(attended)\n",
    "    \n",
    "    # Deep classification head with residual connections\n",
    "    x = layers.Dense(512, activation='relu')(merged)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x2 = layers.Dense(256, activation='relu')(x)\n",
    "    x2 = layers.BatchNormalization()(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "    \n",
    "    # Residual connection\n",
    "    x_res = layers.Dense(256)(x)\n",
    "    x = layers.Add()([x2, x_res])\n",
    "    \n",
    "    outputs = layers.Dense(8, activation='softmax', name='classification')(x)\n",
    "    \n",
    "    return Model(inputs=[acc_in, gyr_in, mag_in, mic_in], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5b9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_class_weights(alpha=None, gamma=2.0, from_logits=False):\n",
    "    \"\"\"\n",
    "    Enhanced Focal Loss with class-specific alpha weights\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Convert to one-hot if needed\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        # Compute probabilities\n",
    "        if from_logits:\n",
    "            y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Clip predictions\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-8, 1 - 1e-8)\n",
    "        \n",
    "        # Get true class probabilities\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=8)\n",
    "        pt = tf.reduce_sum(y_true_one_hot * y_pred, axis=-1)\n",
    "        \n",
    "        # Compute focal weight\n",
    "        focal_weight = tf.pow(1 - pt, gamma)\n",
    "        \n",
    "        # Apply class weights\n",
    "        if alpha is not None:\n",
    "            alpha_t = tf.gather(alpha, y_true)\n",
    "            focal_weight = alpha_t * focal_weight\n",
    "        \n",
    "        # Compute cross entropy\n",
    "        ce = -tf.math.log(pt)\n",
    "        \n",
    "        # Final focal loss\n",
    "        loss = focal_weight * ce\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce0f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enhanced_class_weights(y_train, method='effective_number'):\n",
    "    \"\"\"Compute class weights using effective number method\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    class_counts = Counter(y_train)\n",
    "    n_samples = len(y_train)\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    if method == 'effective_number':\n",
    "        # Effective Number of Samples method\n",
    "        beta = 0.9999\n",
    "        effective_num = 1.0 - np.power(beta, list(class_counts.values()))\n",
    "        weights = (1.0 - beta) / np.array(effective_num)\n",
    "        weights = weights / weights.sum() * n_classes\n",
    "    else:\n",
    "        # Standard balanced weights\n",
    "        weights = n_samples / (n_classes * np.array(list(class_counts.values())))\n",
    "    \n",
    "    return {i: w for i, w in enumerate(weights)}\n",
    "\n",
    "# Compute enhanced weights\n",
    "enhanced_weights = compute_enhanced_class_weights(y_train, method='effective_number')\n",
    "alpha_weights = tf.constant([enhanced_weights[i] for i in range(8)], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36066e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       "array([0.5534995 , 0.10005684, 1.0233018 , 1.0233018 , 1.4255127 ,\n",
       "       1.4255127 , 1.4255127 , 1.0233018 ], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_weights\n",
    "alpha_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a7effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sahoo\\OneDrive\\Desktop\\Master_Seminar\\ADLTS\\tool-tracking\\.env\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ acc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gyr (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mag (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mic (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ acc_branch          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,744</span> │ acc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gyr_branch          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,744</span> │ gyr[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mag_branch          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,744</span> │ mag[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mic_branch          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,592</span> │ mic[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ acc_branch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ gyr_branch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ mag_branch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ mic_branch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ acc (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gyr (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mag (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mic (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ acc_branch          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │     \u001b[38;5;34m39,744\u001b[0m │ acc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gyr_branch          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │     \u001b[38;5;34m39,744\u001b[0m │ gyr[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mag_branch          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │     \u001b[38;5;34m39,744\u001b[0m │ mag[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mic_branch          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │     \u001b[38;5;34m38,592\u001b[0m │ mic[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ acc_branch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ gyr_branch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ mag_branch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ mic_branch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │     \u001b[38;5;34m37,248\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m197,120\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │      \u001b[38;5;34m2,056\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">659,976</span> (2.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m659,976\u001b[0m (2.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,672</span> (2.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m657,672\u001b[0m (2.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> (9.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,304\u001b[0m (9.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shapes = {\n",
    "    'acc': (41, 3),    # 41 timesteps, 3 features\n",
    "    'gyr': (41, 3),\n",
    "    'mag': (62, 3),\n",
    "    'mic': (3200, 1)\n",
    "}\n",
    "\n",
    "# Build and compile\n",
    "model = build_enhanced_fusion_model(input_shapes)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, weight_decay=1e-5),\n",
    "    loss=focal_loss_with_class_weights(alpha=alpha_weights, gamma=2.5),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbcc52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=25,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_enhanced_model.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_auc',\n",
    "        mode='max'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a04d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all 8 classes (0-7) are present in class_weight_dict\n",
    "#missing_classes = set(range(8)) - set(class_weights_dict.keys())\n",
    "#for cls in missing_classes:\n",
    "    #class_weights_dict[cls] = 1.0  # Default neutral weight\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "#class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "#print(\"Updated class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83399717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    }
   ],
   "source": [
    "# Training with validation split\n",
    "history = model.fit(\n",
    "    x={\n",
    "        'acc': X_acc_train,\n",
    "        'gyr': X_gyr_train,\n",
    "        'mag': X_mag_train,\n",
    "        'mic': X_mic_train\n",
    "    },\n",
    "    y=y_train,\n",
    "    validation_split=0.2,  # Essential for monitoring\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc14748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 2.2283\n",
      "Test Accuracy: 0.1818\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(\n",
    "    x={\n",
    "        'acc': X_acc_test,\n",
    "        'gyr': X_gyr_test,\n",
    "        'mag': X_mag_test,\n",
    "        'mic': X_mic_test\n",
    "    },\n",
    "    y=y_test,\n",
    "    batch_size=64,\n",
    "    verbose=0  # Silent mode\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7072ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual classes in test set: [0 1 2 3 4 5 6 7]\n",
      "Classes predicted: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Class 0 (2)       0.13      0.19      0.15       164\n",
      " Class 1 (3)       0.01      0.04      0.02        67\n",
      " Class 2 (4)       0.04      0.10      0.06        60\n",
      " Class 3 (5)       0.02      0.08      0.03        26\n",
      " Class 4 (6)       0.01      0.07      0.01        14\n",
      " Class 5 (7)       0.00      0.00      0.00        17\n",
      " Class 6 (8)       0.68      0.20      0.31      1085\n",
      "Class 7 (14)       0.02      0.33      0.03         3\n",
      "\n",
      "    accuracy                           0.18      1436\n",
      "   macro avg       0.11      0.13      0.08      1436\n",
      "weighted avg       0.53      0.18      0.26      1436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "y_pred_probs = model.predict(\n",
    "    x={\n",
    "        'acc': X_acc_test,\n",
    "        'gyr': X_gyr_test,\n",
    "        'mag': X_mag_test,\n",
    "        'mic': X_mic_test\n",
    "    },\n",
    "    batch_size=64,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Convert to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get unique classes in test set and predictions\n",
    "present_test_classes = np.unique(y_test)\n",
    "present_pred_classes = np.unique(y_pred)\n",
    "\n",
    "print(\"Actual classes in test set:\", present_test_classes)\n",
    "print(\"Classes predicted:\", present_pred_classes)\n",
    "\n",
    "\n",
    "all_classes = np.arange(8)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred,\n",
    "    labels=all_classes,  # Force all classes to be considered\n",
    "    target_names=[\n",
    "        'Class 0 (2)', 'Class 1 (3)', 'Class 2 (4)', 'Class 3 (5)',\n",
    "        'Class 4 (6)', 'Class 5 (7)', 'Class 6 (8)', 'Class 7 (14)'\n",
    "    ]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Create a timestamp for unique file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = f\"results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save Training History\n",
    "print(\"Saving training history...\")\n",
    "# Save as CSV for easy plotting later\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(results_dir, f\"training_history_{timestamp}.csv\"), index=False)\n",
    "\n",
    "# Save training history as formatted text\n",
    "with open(os.path.join(results_dir, f\"training_log_{timestamp}.txt\"), 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"TRAINING HISTORY LOG\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Total Epochs Trained: {len(history.history['loss'])}\\n\")\n",
    "    f.write(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\\n\")\n",
    "    \n",
    "    if 'val_loss' in history.history:\n",
    "        f.write(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Best Validation Loss: {min(history.history['val_loss']):.4f}\\n\")\n",
    "        f.write(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"EPOCH-BY-EPOCH TRAINING LOG:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(len(history.history['loss'])):\n",
    "        f.write(f\"Epoch {epoch+1:3d}: \")\n",
    "        f.write(f\"loss={history.history['loss'][epoch]:.4f}, \")\n",
    "        f.write(f\"acc={history.history['accuracy'][epoch]:.4f}\")\n",
    "        \n",
    "        if 'val_loss' in history.history:\n",
    "            f.write(f\", val_loss={history.history['val_loss'][epoch]:.4f}\")\n",
    "            f.write(f\", val_acc={history.history['val_accuracy'][epoch]:.4f}\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 2. Save Test Results\n",
    "print(\"Saving test results...\")\n",
    "with open(os.path.join(results_dir, f\"test_results_{timestamp}.txt\"), 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"TEST RESULTS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy Percentage: {test_acc*100:.2f}%\\n\")\n",
    "    \n",
    "    # Add dataset information\n",
    "    f.write(f\"\\nDataset Information:\\n\")\n",
    "    f.write(f\"Training samples: {len(y_train)}\\n\")\n",
    "    f.write(f\"Test samples: {len(y_test)}\\n\")\n",
    "    f.write(f\"Number of classes: 8\\n\")\n",
    "    \n",
    "    # Add class distribution in test set\n",
    "    f.write(f\"\\nTest Set Class Distribution:\\n\")\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    for cls, count in zip(unique_test, counts_test):\n",
    "        f.write(f\"Class {cls}: {count} samples\\n\")\n",
    "\n",
    "# 3. Save Classification Report\n",
    "print(\"Saving classification report...\")\n",
    "with open(os.path.join(results_dir, f\"classification_report_{timestamp}.txt\"), 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"DETAILED CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        y_test, \n",
    "        y_pred,\n",
    "        labels=all_classes,\n",
    "        target_names=[\n",
    "            'Class 0 (2)', 'Class 1 (3)', 'Class 2 (4)', 'Class 3 (5)',\n",
    "            'Class 4 (6)', 'Class 5 (7)', 'Class 6 (8)', 'Class 7 (14)'\n",
    "        ]\n",
    "    )\n",
    "    f.write(class_report)\n",
    "    \n",
    "    # Add confusion matrix\n",
    "    f.write(f\"\\n\\nConfusion Matrix:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_classes)\n",
    "    f.write(\"Predicted →\\n\")\n",
    "    f.write(\"Actual ↓     \")\n",
    "    for i in range(8):\n",
    "        f.write(f\"{i:4d}\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    for i, row in enumerate(cm):\n",
    "        f.write(f\"Class {i}:   \")\n",
    "        for val in row:\n",
    "            f.write(f\"{val:4d}\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Add per-class accuracy\n",
    "    f.write(f\"\\nPer-Class Accuracy:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, acc in enumerate(class_accuracies):\n",
    "        if cm.sum(axis=1)[i] > 0:  # Only show classes present in test set\n",
    "            f.write(f\"Class {i}: {acc:.4f} ({acc*100:.2f}%)\\n\")\n",
    "\n",
    "# 4. Save Model Configuration Summary\n",
    "print(\"Saving model configuration...\")\n",
    "with open(os.path.join(results_dir, f\"model_config_{timestamp}.txt\"), 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"MODEL CONFIGURATION\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Input Shapes:\\n\")\n",
    "    for sensor, shape in input_shapes.items():\n",
    "        f.write(f\"  {sensor}: {shape}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nModel Parameters:\\n\")\n",
    "    f.write(f\"  Total parameters: {model.count_params():,}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTraining Configuration:\\n\")\n",
    "    f.write(f\"  Optimizer: Adam (lr=1e-4, weight_decay=1e-5)\\n\")\n",
    "    f.write(f\"  Loss: Focal Loss with class weights (gamma=2.5)\\n\")\n",
    "    f.write(f\"  Batch size: 64\\n\")\n",
    "    f.write(f\"  Max epochs: 80\\n\")\n",
    "    f.write(f\"  Early stopping patience: 25\\n\")\n",
    "\n",
    "# 5. Save Resampling Information\n",
    "print(\"Saving resampling information...\")\n",
    "with open(os.path.join(results_dir, f\"resampling_info_{timestamp}.txt\"), 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"DATA RESAMPLING INFORMATION\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Original Class Distribution:\\n\")\n",
    "    original_counts = {0: 657, 1: 268, 2: 238, 3: 106, 4: 54, 5: 70, 6: 4335, 7: 13}\n",
    "    for cls, count in original_counts.items():\n",
    "        f.write(f\"  Class {cls}: {count} samples\\n\")\n",
    "    \n",
    "    f.write(f\"\\nResampled Class Distribution:\\n\")\n",
    "    unique_resampled, counts_resampled = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique_resampled, counts_resampled):\n",
    "        f.write(f\"  Class {cls}: {count} samples\\n\")\n",
    "    \n",
    "    f.write(f\"\\nResampling Method: ADASYN\\n\")\n",
    "    f.write(f\"Total samples after resampling: {len(y_train)}\\n\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"All results saved to directory: {results_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - training_history_{timestamp}.csv\")\n",
    "print(f\"  - training_log_{timestamp}.txt\")\n",
    "print(f\"  - test_results_{timestamp}.txt\")\n",
    "print(f\"  - classification_report_{timestamp}.txt\")\n",
    "print(f\"  - model_config_{timestamp}.txt\")\n",
    "print(f\"  - resampling_info_{timestamp}.txt\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
